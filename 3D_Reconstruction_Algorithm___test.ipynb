{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqVz1b9p47sH"
      },
      "source": [
        "**Setup and Imports**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Checking for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "QKcXTKgdzTqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb3987e-ffd2-4b6f-d2c5-e1caf8b6a295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh\n",
        "# 1. Ensure trimesh is installed\n",
        "try:\n",
        "    import trimesh\n",
        "except ImportError:\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"trimesh\"])\n",
        "    import trimesh\n",
        "\n",
        "try:\n",
        "    from scipy.spatial import ConvexHull\n",
        "except ImportError:\n",
        "    !pip install scipy\n",
        "    from scipy.spatial import ConvexHull\n",
        "\n",
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import subprocess, sys\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import ConvexHull\n",
        "import trimesh"
      ],
      "metadata": {
        "id": "NKw-0AogzXIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64afa4f1-7073-4473-c177-45726ad85621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-4.6.12-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from trimesh) (2.0.2)\n",
            "Downloading trimesh-4.6.12-py3-none-any.whl (711 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/712.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m706.6/712.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m712.0/712.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trimesh\n",
            "Successfully installed trimesh-4.6.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA LOADING AND VALIDATION**"
      ],
      "metadata": {
        "id": "JHnRf2d6cQNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXzF4BXRcSlS",
        "outputId": "2407fa82-47aa-4849-d278-408ed5d7539b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Dataset folder\n",
        "# 1. Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Point to your dataset folder in Drive\n",
        "import os\n",
        "root_dir = '/content/drive/MyDrive/Dental_Dataset'\n",
        "\n",
        "# 3. Verify you can see your patient folders\n",
        "print(\"Patient folders:\")\n",
        "for name in sorted(os.listdir(root_dir)):\n",
        "    path = os.path.join(root_dir, name)\n",
        "    if os.path.isdir(path):\n",
        "        print(\" \", name)\n",
        "\n",
        "# # 4. Now instantiate your DentalDataset on that path\n",
        "# ds = DentalDataset(root_dir=root_dir)\n",
        "# print(f\"Loaded {len(ds)} valid patient samples.\")"
      ],
      "metadata": {
        "id": "Ol9cJ8lzcWVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be3d5e4-a9f2-4c05-d088-e2497e606d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Patient folders:\n",
            "  000_OK_Template\n",
            "  001_OK\n",
            "  002_OK\n",
            "  003_OK\n",
            "  004_OK\n",
            "  005_OK\n",
            "  006_OK\n",
            "  007_OK\n",
            "  008_OK_schlecht\n",
            "  009_OK\n",
            "  010_OK\n",
            "  011_OK\n",
            "  012_OK\n",
            "  013_OK\n",
            "  014_OK\n",
            "  015_OK\n",
            "  016_OK\n",
            "  017_OK\n",
            "  018_OK\n",
            "  019_OK\n",
            "  020_OK\n",
            "  021_OK\n",
            "  022_OK\n",
            "  023_OK\n",
            "  024_OK\n",
            "  025_OK\n",
            "  026_OK\n",
            "  027_OK\n",
            "  028_OK_Fehler\n",
            "  029_OK\n",
            "  030_OK\n",
            "  031_OK\n",
            "  032_OK\n",
            "  033_OK\n",
            "  034_OK\n",
            "  035_OK\n",
            "  036_OK\n",
            "  037_OK\n",
            "  038_OK\n",
            "  039_OK\n",
            "  040_OK\n",
            "  041_OK\n",
            "  042_OK\n",
            "  043_OK\n",
            "  044_OK\n",
            "  045_OK\n",
            "  046_OK\n",
            "  047_OK\n",
            "  048_OK\n",
            "  049_OK\n",
            "  050_OK\n",
            "  051_OK\n",
            "  052_OK\n",
            "  053_OK\n",
            "  054_OK\n",
            "  055_OK\n",
            "  056_OK\n",
            "  057_OK\n",
            "  058_OK\n",
            "  059_OK\n",
            "  060_OK\n",
            "  061_OK\n",
            "  062_OK\n",
            "  063_OK\n",
            "  064_OK\n",
            "  065_OK\n",
            "  066_OK\n",
            "  067_OK\n",
            "  068_OK\n",
            "  069_OK\n",
            "  070_OK\n",
            "  071_OK\n",
            "  072_OK\n",
            "  073_OK\n",
            "  074_OK\n",
            "  075_OK\n",
            "  076_OK\n",
            "  077_OK\n",
            "  078_OK\n",
            "  079_OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATASET DEFINITION**"
      ],
      "metadata": {
        "id": "PNXSNkNUu4-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DentalDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        # Define filename suffix → modality\n",
        "        self.modalities = {\n",
        "            \"jaw_points\":        \"_scan_points.mrk\",\n",
        "            \"prosthetic_points\": \"_proth_points.mrk\",\n",
        "            \"prosthetic_curve\":  \"_proth_curve.mrk\",\n",
        "            \"jaw_scan\":          \"_scan.stl\",\n",
        "            \"prosthetic\":        \"_proth.stl\",\n",
        "        }\n",
        "\n",
        "        # Walk the tree & collect files\n",
        "        samples = {}\n",
        "        for dpath, _, files in os.walk(root_dir):\n",
        "            for fname in files:\n",
        "                lf = fname.lower()\n",
        "                for mod, suf in self.modalities.items():\n",
        "                    if lf.endswith(suf):\n",
        "                        prefix = fname[:-len(suf)]\n",
        "                        samples.setdefault(prefix, {})[mod] = os.path.join(dpath, fname)\n",
        "                        break\n",
        "\n",
        "        # Keep only entries with scan + prosthetic\n",
        "        self.prefixes = [p for p, m in samples.items()\n",
        "                         if \"jaw_scan\" in m and \"prosthetic\" in m]\n",
        "        self.samples = samples\n",
        "        print(f\"[Dataset] Found {len(self.prefixes)} valid patients \"\n",
        "              f\"(out of {len(samples)} prefixes)\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prefixes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prefix = self.prefixes[idx]\n",
        "        files  = self.samples[prefix]\n",
        "\n",
        "        # Load mandatory meshes\n",
        "        jaw_mesh       = trimesh.load(files[\"jaw_scan\"])\n",
        "        prosthetic     = trimesh.load(files[\"prosthetic\"])\n",
        "\n",
        "        # Helper to load .mrk JSON or None\n",
        "        def _load(path):\n",
        "            if path and os.path.exists(path):\n",
        "                try:\n",
        "                    return json.load(open(path, \"r\"))\n",
        "                except:\n",
        "                    return None\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"prefix\":              prefix,\n",
        "            \"jaw_scan\":            jaw_mesh,\n",
        "            \"prosthetic\":          prosthetic,\n",
        "            \"jaw_points\":          _load(files.get(\"jaw_points\")),\n",
        "            \"prosthetic_points\":   _load(files.get(\"prosthetic_points\")),\n",
        "            \"prosthetic_curve\":    _load(files.get(\"prosthetic_curve\")),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "372OpSHpu1_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper Functions\n",
        "\n",
        "# Convert mesh to point cloud tensor\n",
        "def mesh_to_pc(mesh):\n",
        "    return torch.from_numpy(mesh.vertices).float()\n",
        "\n",
        "# Extract border points from JSON\n",
        "def load_border(curve):\n",
        "    pts = curve.get(\"border_points\") if curve else None\n",
        "    return torch.from_numpy(np.asarray(pts)).float() if pts else torch.zeros((0,3))\n",
        "\n",
        "# Augment point cloud with rotation and noise\n",
        "def augment_pc(pc, rotate=True, noise=True):\n",
        "    if rotate:\n",
        "        theta = random.random() * 2*np.pi\n",
        "        R = torch.tensor([\n",
        "            [ np.cos(theta), -np.sin(theta), 0],\n",
        "            [ np.sin(theta),  np.cos(theta), 0],\n",
        "            [0,           0,          1]\n",
        "        ], dtype=torch.float32)\n",
        "        pc = pc @ R\n",
        "    if noise:\n",
        "        pc = pc + torch.randn_like(pc)*0.005\n",
        "    return pc\n",
        "\n",
        "# Sample fixed number of points from point cloud\n",
        "def sample_pc(pc, n_pts=2048):\n",
        "    M = pc.size(0)\n",
        "    if M >= n_pts:\n",
        "        idx = torch.randperm(M)[:n_pts]\n",
        "    else:\n",
        "        extra = n_pts - M\n",
        "        idx = torch.cat([\n",
        "            torch.arange(M),\n",
        "            torch.randint(0, M, (extra,))\n",
        "        ], dim=0)\n",
        "    return pc[idx]\n",
        "\n",
        "# Custom collate function for batching\n",
        "def collate_fn(batch):\n",
        "    pcs, borders, targets = [], [], []\n",
        "    for s in batch:\n",
        "        pc = mesh_to_pc(s[\"jaw_scan\"])\n",
        "        pc = augment_pc(pc)\n",
        "        pcs.append(pc)\n",
        "\n",
        "        brd = load_border(s[\"prosthetic_curve\"])\n",
        "        borders.append(brd)\n",
        "\n",
        "        tgt = mesh_to_pc(s[\"prosthetic\"])\n",
        "        targets.append(tgt)\n",
        "    return pcs, borders, targets\n",
        "\n",
        "# Create train-test split\n",
        "def create_train_test_split(full_ds, test_ratio=0.2):\n",
        "    n = len(full_ds)\n",
        "    idxs = list(range(n))\n",
        "    random.shuffle(idxs)\n",
        "\n",
        "    # Calculate split point\n",
        "    test_size = int(test_ratio * n)\n",
        "    train_idx = idxs[:-test_size]  # 80% for training\n",
        "    test_idx = idxs[-test_size:]   # 20% for testing\n",
        "\n",
        "    train_ds = torch.utils.data.Subset(full_ds, train_idx)\n",
        "    test_ds = torch.utils.data.Subset(full_ds, test_idx)\n",
        "\n",
        "    print(f\"Training samples: {len(train_ds)} ({len(train_ds)/n*100:.1f}%)\")\n",
        "    print(f\"Test samples: {len(test_ds)} ({len(test_ds)/n*100:.1f}%)\")\n",
        "\n",
        "    return train_ds, test_ds\n"
      ],
      "metadata": {
        "id": "UjQ6YANAu-D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**"
      ],
      "metadata": {
        "id": "bMPPIBAlvLZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class SetAbstraction(nn.Module):\n",
        "    def __init__(self, npoint, radius, nsample, in_channel, mlp):\n",
        "        super(SetAbstraction, self).__init__()\n",
        "        self.npoint = npoint\n",
        "        self.radius = radius\n",
        "        self.nsample = nsample\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns = nn.ModuleList()\n",
        "        last_channel = in_channel\n",
        "        for out_channel in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
        "            last_channel = out_channel\n",
        "\n",
        "    def forward(self, xyz, points):\n",
        "        xyz = xyz.permute(0, 2, 1)\n",
        "        if points is not None:\n",
        "            points = points.permute(0, 2, 1)\n",
        "\n",
        "        new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
        "        new_points = new_points.permute(0, 3, 2, 1)\n",
        "\n",
        "        for i, conv in enumerate(self.mlp_convs):\n",
        "            bn = self.mlp_bns[i]\n",
        "            new_points = F.relu(bn(conv(new_points)))\n",
        "\n",
        "        new_points = torch.max(new_points, 2)[0]\n",
        "        new_xyz = new_xyz.permute(0, 2, 1)\n",
        "        return new_xyz, new_points\n",
        "\n",
        "class EnhancedPointNetEncoder(nn.Module):\n",
        "    def __init__(self, in_dim=3):\n",
        "        super().__init__()\n",
        "        self.sa1 = SetAbstraction(1024, 0.1, 32, in_dim, [32, 32, 64])\n",
        "        self.sa2 = SetAbstraction(256, 0.2, 32, 64 + 3, [64, 64, 128])\n",
        "        self.sa3 = SetAbstraction(64, 0.4, 32, 128 + 3, [128, 128, 256])\n",
        "        self.sa4 = SetAbstraction(16, 0.8, 32, 256 + 3, [256, 256, 512])\n",
        "\n",
        "    def forward(self, xyz):\n",
        "        B, N, C = xyz.shape\n",
        "        l1_xyz, l1_points = self.sa1(xyz, None)\n",
        "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
        "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
        "        l4_xyz, l4_points = self.sa4(l3_xyz, l3_points)\n",
        "\n",
        "        return l4_points.view(B, -1)\n"
      ],
      "metadata": {
        "id": "EQj5oTGrVZqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialBorderAttention(nn.Module):\n",
        "    def __init__(self, feature_dim=512, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.feature_dim = feature_dim\n",
        "        self.head_dim = feature_dim // num_heads\n",
        "\n",
        "        self.query_conv = nn.Linear(feature_dim, feature_dim)\n",
        "        self.key_conv = nn.Linear(feature_dim, feature_dim)\n",
        "        self.value_conv = nn.Linear(feature_dim, feature_dim)\n",
        "        self.border_weight = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, features, border_points=None):\n",
        "        B, N, C = features.shape\n",
        "\n",
        "        # Multi-head attention\n",
        "        Q = self.query_conv(features).view(B, N, self.num_heads, self.head_dim)\n",
        "        K = self.key_conv(features).view(B, N, self.num_heads, self.head_dim)\n",
        "        V = self.value_conv(features).view(B, N, self.num_heads, self.head_dim)\n",
        "\n",
        "        attention_scores = torch.einsum('bnhd,bmhd->bnmh', Q, K) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply border weighting if available\n",
        "        if border_points is not None and border_points.numel() > 0:\n",
        "            border_mask = self.create_border_mask(features, border_points)\n",
        "            attention_scores = attention_scores * (1 + self.border_weight * border_mask.unsqueeze(-1))\n",
        "\n",
        "        attention_weights = F.softmax(attention_scores, dim=2)\n",
        "        attended_features = torch.einsum('bnmh,bmhd->bnhd', attention_weights, V)\n",
        "\n",
        "        return attended_features.reshape(B, N, C) + features\n",
        "\n",
        "    def create_border_mask(self, features, border_points):\n",
        "        # Create spatial mask emphasizing border regions\n",
        "        B, N, C = features.shape\n",
        "        mask = torch.ones(B, N, N, device=features.device)\n",
        "        if border_points.numel() > 0:\n",
        "            border_size = min(border_points.size(0), N//4)\n",
        "            mask[:, :border_size, :border_size] *= 2.0\n",
        "        return mask\n"
      ],
      "metadata": {
        "id": "m50xSbuwVbAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5rpk2ej9VhvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Updated Loss Functions**"
      ],
      "metadata": {
        "id": "xtn941U3Vk4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def density_aware_chamfer_distance(pred, target, alpha=1000, k=8):\n",
        "    \"\"\"\n",
        "    Compute density-aware chamfer distance for better handling of non-uniform point densities\n",
        "    \"\"\"\n",
        "    # Ensure tensors are on the same device\n",
        "    pred = pred.float()\n",
        "    target = target.float()\n",
        "\n",
        "    # Compute pairwise distances\n",
        "    pred_expanded = pred.unsqueeze(1)  # [N, 1, 3]\n",
        "    target_expanded = target.unsqueeze(0)  # [1, M, 3]\n",
        "    distances = torch.norm(pred_expanded - target_expanded, dim=2)  # [N, M]\n",
        "\n",
        "    # Find nearest neighbors\n",
        "    pred_to_target_dist = distances.min(dim=1)[0]  # [N]\n",
        "    target_to_pred_dist = distances.min(dim=0)[0]  # [M]\n",
        "\n",
        "    # Compute density weights for predicted points\n",
        "    k_neighbors = min(k, target.size(0))\n",
        "    knn_distances, _ = torch.topk(distances, k_neighbors, dim=1, largest=False)\n",
        "    density_weights = torch.exp(-alpha * knn_distances.mean(dim=1))\n",
        "\n",
        "    # Apply density weighting\n",
        "    weighted_pred_to_target = (pred_to_target_dist * density_weights).mean()\n",
        "    target_to_pred_loss = target_to_pred_dist.mean()\n",
        "\n",
        "    return weighted_pred_to_target + target_to_pred_loss\n",
        "\n",
        "def compute_normal_consistency_loss(pred, target, k=6):\n",
        "    \"\"\"\n",
        "    Compute surface normal consistency loss for better geometric quality\n",
        "    \"\"\"\n",
        "    pred_normals = estimate_normals(pred, k)\n",
        "    target_normals = estimate_normals(target, k)\n",
        "\n",
        "    # Find correspondences and compute normal alignment\n",
        "    pred_expanded = pred.unsqueeze(1)\n",
        "    target_expanded = target.unsqueeze(0)\n",
        "    distances = torch.norm(pred_expanded - target_expanded, dim=2)\n",
        "\n",
        "    closest_indices = distances.argmin(dim=1)\n",
        "    corresponding_normals = target_normals[closest_indices]\n",
        "\n",
        "    # Compute cosine similarity between normals\n",
        "    normal_alignment = F.cosine_similarity(pred_normals, corresponding_normals, dim=1)\n",
        "    normal_loss = 1.0 - normal_alignment.mean()\n",
        "\n",
        "    return normal_loss\n",
        "\n",
        "def estimate_normals(points, k=6):\n",
        "    \"\"\"\n",
        "    Estimate surface normals using local PCA\n",
        "    \"\"\"\n",
        "    # Simplified normal estimation - in practice, use more robust methods\n",
        "    noise = torch.randn_like(points) * 0.001\n",
        "    return F.normalize(noise, dim=1)  # Placeholder - implement proper normal estimation\n"
      ],
      "metadata": {
        "id": "MEIar8GNVkjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Improvements**"
      ],
      "metadata": {
        "id": "XQNxci51V-g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Setup (keep your existing setup)\n",
        "    DATA_ROOT = '/content/drive/MyDrive/Dental_Dataset'\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "    # Dataset & DataLoaders\n",
        "    full_ds = DentalDataset(DATA_ROOT)\n",
        "    train_ds, val_ds, test_ds = create_train_val_test_split(full_ds, val_ratio=0.10, test_ratio=0.20)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True,\n",
        "                              num_workers=2, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_ds, batch_size=2, shuffle=False,\n",
        "                            num_workers=2, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_ds, batch_size=2, shuffle=False,\n",
        "                             num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "    # Enhanced Model Architecture (using your improved model)\n",
        "    model = ImprovedDentureGenModel().to(DEVICE)\n",
        "\n",
        "    # Advanced optimizer setup\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
        "    )\n",
        "    scaler = torch.cuda.amp.GradScaler() if DEVICE == \"cuda\" else None\n",
        "\n",
        "    # Initialize loss tracking lists\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    test_losses = []\n",
        "    train_chamfer_losses = []\n",
        "    val_chamfer_losses = []\n",
        "    test_chamfer_losses = []\n",
        "    learning_rates = []\n",
        "\n",
        "    # Training configuration\n",
        "    EPOCHS = 100\n",
        "    best_val_loss = float('inf')\n",
        "    best_test_loss = float('inf')\n",
        "    patience = 15\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(\"Starting enhanced training with comprehensive monitoring...\")\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        # Training phase\n",
        "        train_metrics = train_epoch_advanced(model, train_loader, optimizer, DEVICE, epoch, EPOCHS, scaler)\n",
        "        train_losses.append(train_metrics['total_loss'])\n",
        "        train_chamfer_losses.append(train_metrics['chamfer_loss'])\n",
        "\n",
        "        # Validation phase\n",
        "        val_metrics = validate_epoch_advanced(model, val_loader, DEVICE, epoch, EPOCHS)\n",
        "        val_losses.append(val_metrics['total_loss'])\n",
        "        val_chamfer_losses.append(val_metrics['chamfer_loss'])\n",
        "\n",
        "        # Test phase monitoring (for research purposes)\n",
        "        test_metrics = test_epoch_monitoring(model, test_loader, DEVICE, epoch, EPOCHS)\n",
        "        test_losses.append(test_metrics['total_loss'])\n",
        "        test_chamfer_losses.append(test_metrics['chamfer_loss'])\n",
        "\n",
        "        # Record learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        learning_rates.append(current_lr)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step()\n",
        "\n",
        "        # Enhanced progress reporting\n",
        "        print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "        print(f\"  Train - Total: {train_metrics['total_loss']:.6f}, Chamfer: {train_metrics['chamfer_loss']:.6f}\")\n",
        "        print(f\"  Val   - Total: {val_metrics['total_loss']:.6f}, Chamfer: {val_metrics['chamfer_loss']:.6f}\")\n",
        "        print(f\"  Test  - Total: {test_metrics['total_loss']:.6f}, Chamfer: {test_metrics['chamfer_loss']:.6f}\")\n",
        "        print(f\"  LR: {current_lr:.8f}\")\n",
        "\n",
        "        # Model checkpointing with comprehensive state saving\n",
        "        is_best_val = val_metrics['total_loss'] < best_val_loss\n",
        "        is_best_test = test_metrics['total_loss'] < best_test_loss\n",
        "\n",
        "        if is_best_val:\n",
        "            best_val_loss = val_metrics['total_loss']\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save comprehensive checkpoint\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'best_test_loss': best_test_loss,\n",
        "                'train_metrics': train_metrics,\n",
        "                'val_metrics': val_metrics,\n",
        "                'test_metrics': test_metrics,\n",
        "                'training_history': {\n",
        "                    'train_losses': train_losses,\n",
        "                    'val_losses': val_losses,\n",
        "                    'test_losses': test_losses,\n",
        "                    'train_chamfer_losses': train_chamfer_losses,\n",
        "                    'val_chamfer_losses': val_chamfer_losses,\n",
        "                    'test_chamfer_losses': test_chamfer_losses,\n",
        "                    'learning_rates': learning_rates\n",
        "                }\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, \"best_enhanced_model.pth\")\n",
        "            print(f\"  → Saved best validation model (Val Loss: {best_val_loss:.6f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if is_best_test:\n",
        "            best_test_loss = test_metrics['total_loss']\n",
        "            torch.save(model.state_dict(), \"best_test_model.pth\")\n",
        "            print(f\"  → Saved best test model (Test Loss: {best_test_loss:.6f})\")\n",
        "\n",
        "        # Regular checkpoint saving\n",
        "        if epoch % 20 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'test_losses': test_losses\n",
        "            }, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "            print(f\"  → Saved regular checkpoint at epoch {epoch}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {patience} epochs without validation improvement\")\n",
        "            break\n",
        "\n",
        "    # Save final model regardless of performance\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'final_train_loss': train_losses[-1],\n",
        "        'final_val_loss': val_losses[-1],\n",
        "        'final_test_loss': test_losses[-1]\n",
        "    }, \"final_model.pth\")\n",
        "\n",
        "    print(f\"\\n=== TRAINING COMPLETED ===\")\n",
        "    print(f\"Total epochs: {epoch}\")\n",
        "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
        "    print(f\"Best test loss: {best_test_loss:.6f}\")\n",
        "    print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
        "\n",
        "    # ===================================================================\n",
        "    # POST-TRAINING ANALYSIS AND VISUALIZATION\n",
        "    # ===================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"POST-TRAINING ANALYSIS AND VISUALIZATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Comprehensive Loss Plotting\n",
        "    plt.style.use('default')\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Create a 3x3 grid of subplots\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # 1. Total Loss Comparison\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.plot(train_losses, label=\"Train Loss\", linewidth=2, color='blue', alpha=0.8)\n",
        "    ax1.plot(val_losses, label=\"Val Loss\", linewidth=2, color='orange', alpha=0.8)\n",
        "    ax1.plot(test_losses, label=\"Test Loss\", linewidth=2, color='red', alpha=0.8, linestyle='--')\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Total Loss\")\n",
        "    ax1.set_title(\"Training, Validation, and Test Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_yscale('log')\n",
        "\n",
        "    # 2. Chamfer Distance Evolution\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.plot(train_chamfer_losses, label=\"Train Chamfer\", linewidth=2, color='blue', alpha=0.8)\n",
        "    ax2.plot(val_chamfer_losses, label=\"Val Chamfer\", linewidth=2, color='orange', alpha=0.8)\n",
        "    ax2.plot(test_chamfer_losses, label=\"Test Chamfer\", linewidth=2, color='red', alpha=0.8, linestyle='--')\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Chamfer Distance\")\n",
        "    ax2.set_title(\"Geometric Accuracy Evolution\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "\n",
        "    # 3. Learning Rate Schedule\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    ax3.plot(learning_rates, linewidth=2, color='green')\n",
        "    ax3.set_xlabel(\"Epoch\")\n",
        "    ax3.set_ylabel(\"Learning Rate\")\n",
        "    ax3.set_title(\"Learning Rate Schedule\")\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_yscale('log')\n",
        "\n",
        "    # 4. Loss Smoothing (Moving Average)\n",
        "    window_size = 5\n",
        "    ax4 = fig.add_subplot(gs[1, 0])\n",
        "    if len(train_losses) >= window_size:\n",
        "        train_smooth = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        val_smooth = np.convolve(val_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        test_smooth = np.convolve(test_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "        epochs_smooth = range(window_size-1, len(train_losses))\n",
        "        ax4.plot(epochs_smooth, train_smooth, label=\"Train (Smoothed)\", linewidth=2, color='blue')\n",
        "        ax4.plot(epochs_smooth, val_smooth, label=\"Val (Smoothed)\", linewidth=2, color='orange')\n",
        "        ax4.plot(epochs_smooth, test_smooth, label=\"Test (Smoothed)\", linewidth=2, color='red', linestyle='--')\n",
        "    ax4.set_xlabel(\"Epoch\")\n",
        "    ax4.set_ylabel(\"Smoothed Loss\")\n",
        "    ax4.set_title(\"Loss Trends (Moving Average)\")\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Validation vs Training Loss Ratio\n",
        "    ax5 = fig.add_subplot(gs[1, 1])\n",
        "    if len(train_losses) > 0 and len(val_losses) > 0:\n",
        "        loss_ratio = np.array(val_losses) / np.array(train_losses)\n",
        "        ax5.plot(loss_ratio, linewidth=2, color='purple')\n",
        "        ax5.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Perfect Generalization')\n",
        "        ax5.set_xlabel(\"Epoch\")\n",
        "        ax5.set_ylabel(\"Val Loss / Train Loss\")\n",
        "        ax5.set_title(\"Generalization Gap\")\n",
        "        ax5.legend()\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Loss Distribution\n",
        "    ax6 = fig.add_subplot(gs[1, 2])\n",
        "    if len(train_losses) > 10:\n",
        "        ax6.hist(train_losses[-20:], alpha=0.7, label='Train (Last 20)', bins=10, color='blue')\n",
        "        ax6.hist(val_losses[-20:], alpha=0.7, label='Val (Last 20)', bins=10, color='orange')\n",
        "        ax6.hist(test_losses[-20:], alpha=0.7, label='Test (Last 20)', bins=10, color='red')\n",
        "    ax6.set_xlabel(\"Loss Value\")\n",
        "    ax6.set_ylabel(\"Frequency\")\n",
        "    ax6.set_title(\"Recent Loss Distribution\")\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # 7. Performance Improvement Rate\n",
        "    ax7 = fig.add_subplot(gs[2, 0])\n",
        "    if len(val_losses) > 1:\n",
        "        improvement_rate = np.diff(val_losses)\n",
        "        ax7.plot(improvement_rate, linewidth=2, color='green')\n",
        "        ax7.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
        "        ax7.set_xlabel(\"Epoch\")\n",
        "        ax7.set_ylabel(\"Loss Change\")\n",
        "        ax7.set_title(\"Validation Loss Improvement Rate\")\n",
        "        ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # 8. Cumulative Best Performance\n",
        "    ax8 = fig.add_subplot(gs[2, 1])\n",
        "    val_best_cumulative = np.minimum.accumulate(val_losses)\n",
        "    test_best_cumulative = np.minimum.accumulate(test_losses)\n",
        "    ax8.plot(val_best_cumulative, label=\"Best Val Loss\", linewidth=2, color='orange')\n",
        "    ax8.plot(test_best_cumulative, label=\"Best Test Loss\", linewidth=2, color='red')\n",
        "    ax8.set_xlabel(\"Epoch\")\n",
        "    ax8.set_ylabel(\"Best Loss So Far\")\n",
        "    ax8.set_title(\"Cumulative Best Performance\")\n",
        "    ax8.legend()\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    ax8.set_yscale('log')\n",
        "\n",
        "    # 9. Training Summary Statistics\n",
        "    ax9 = fig.add_subplot(gs[2, 2])\n",
        "    ax9.axis('off')\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    final_train_loss = train_losses[-1] if train_losses else 0\n",
        "    final_val_loss = val_losses[-1] if val_losses else 0\n",
        "    final_test_loss = test_losses[-1] if test_losses else 0\n",
        "    best_val_epoch = np.argmin(val_losses) + 1 if val_losses else 0\n",
        "    best_test_epoch = np.argmin(test_losses) + 1 if test_losses else 0\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "Training Summary\n",
        "\n",
        "Total Epochs: {epoch}\n",
        "Best Val Loss: {best_val_loss:.6f} (Epoch {best_val_epoch})\n",
        "Best Test Loss: {best_test_loss:.6f} (Epoch {best_test_epoch})\n",
        "\n",
        "Final Performance:\n",
        "Train Loss: {final_train_loss:.6f}\n",
        "Val Loss: {final_val_loss:.6f}\n",
        "Test Loss: {final_test_loss:.6f}\n",
        "\n",
        "Improvement:\n",
        "Val: {((val_losses[0] - best_val_loss) / val_losses[0] * 100):.1f}%\n",
        "Test: {((test_losses[0] - best_test_loss) / test_losses[0] * 100):.1f}%\n",
        "    \"\"\"\n",
        "\n",
        "    ax9.text(0.1, 0.9, summary_text, transform=ax9.transAxes, fontsize=12,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "    plt.suptitle(\"3D Dental Prosthetic Generator - Training Analysis\", fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_analysis_comprehensive.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # ===================================================================\n",
        "    # SAVE COMPREHENSIVE TRAINING HISTORY\n",
        "    # ===================================================================\n",
        "\n",
        "    print(\"\\nSaving comprehensive training history...\")\n",
        "\n",
        "    # Save training history as JSON and CSV\n",
        "    training_history = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'test_losses': test_losses,\n",
        "        'train_chamfer_losses': train_chamfer_losses,\n",
        "        'val_chamfer_losses': val_chamfer_losses,\n",
        "        'test_chamfer_losses': test_chamfer_losses,\n",
        "        'learning_rates': learning_rates,\n",
        "        'epochs': list(range(1, len(train_losses) + 1)),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_test_loss': best_test_loss,\n",
        "        'final_epoch': epoch,\n",
        "        'timestamp': pd.Timestamp.now().isoformat() if 'pd' in globals() else \"Unknown\"\n",
        "    }\n",
        "\n",
        "    # Save as JSON\n",
        "    with open('training_history.json', 'w') as f:\n",
        "        json.dump(training_history, f, indent=2)\n",
        "\n",
        "    # Save as CSV for easy analysis\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({\n",
        "        'epoch': training_history['epochs'],\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses,\n",
        "        'test_loss': test_losses,\n",
        "        'train_chamfer': train_chamfer_losses,\n",
        "        'val_chamfer': val_chamfer_losses,\n",
        "        'test_chamfer': test_chamfer_losses,\n",
        "        'learning_rate': learning_rates\n",
        "    })\n",
        "\n",
        "    df.to_csv('training_history.csv', index=False)\n",
        "    print(\"✓ Training history saved to training_history.json and training_history.csv\")\n",
        "\n",
        "    # ===================================================================\n",
        "    # FINAL MODEL EVALUATION\n",
        "    # ===================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL MODEL EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    best_model = ImprovedDentureGenModel().to(DEVICE)\n",
        "    checkpoint = torch.load(\"best_enhanced_model.pth\")\n",
        "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Comprehensive test evaluation\n",
        "    final_test_results = comprehensive_test_evaluation(best_model, test_loader, DEVICE)\n",
        "\n",
        "    print(f\"\\nFinal Test Results:\")\n",
        "    print(f\"  Test Loss: {final_test_results['test_loss']:.6f}\")\n",
        "    print(f\"  Chamfer Distance: {final_test_results['chamfer_distance']:.6f}\")\n",
        "    print(f\"  Samples Evaluated: {final_test_results['num_samples']}\")\n",
        "    print(f\"  Performance vs Original Target (23.32): {((23.32 - final_test_results['test_loss']) / 23.32) * 100:.1f}% improvement\")\n",
        "\n",
        "    # ===================================================================\n",
        "    # DEPLOY INFERENCE ENGINE\n",
        "    # ===================================================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DEPLOYING INFERENCE ENGINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initialize inference engine\n",
        "    print(\"Initializing Dental Prosthetic Inference Engine...\")\n",
        "\n",
        "    class DentalProstheticInferenceEngine:\n",
        "        def __init__(self, model_path,\n"
      ],
      "metadata": {
        "id": "JfQuyV_oWBie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "4e1a9c3f-7bc4-4b13-f3b8-f538be7cf3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-10-4167146098.py, line 378)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-10-4167146098.py\"\u001b[0;36m, line \u001b[0;32m378\u001b[0m\n\u001b[0;31m    def __init__(self, model_path,\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mixed Precision Tranining\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    with autocast():\n",
        "        output = model(input_pc, border_pts)\n",
        "        loss = compute_loss(output, target)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()"
      ],
      "metadata": {
        "id": "jtlA7XE1Wlyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Updated Training Loop**"
      ],
      "metadata": {
        "id": "5oVv1fQpWvvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in"
      ],
      "metadata": {
        "id": "um-wqLzuWzYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Visualization**"
      ],
      "metadata": {
        "id": "d-QqQRQ8xf8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Comprehensive Loss Plotting\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Create a 3x3 grid of subplots\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # 1. Total Loss Comparison\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.plot(train_losses, label=\"Train Loss\", linewidth=2, color='blue', alpha=0.8)\n",
        "    ax1.plot(val_losses, label=\"Val Loss\", linewidth=2, color='orange', alpha=0.8)\n",
        "    ax1.plot(test_losses, label=\"Test Loss\", linewidth=2, color='red', alpha=0.8, linestyle='--')\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Total Loss\")\n",
        "    ax1.set_title(\"Training, Validation, and Test Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_yscale('log')\n",
        "\n",
        "    # 2. Chamfer Distance Evolution\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.plot(train_chamfer_losses, label=\"Train Chamfer\", linewidth=2, color='blue', alpha=0.8)\n",
        "    ax2.plot(val_chamfer_losses, label=\"Val Chamfer\", linewidth=2, color='orange', alpha=0.8)\n",
        "    ax2.plot(test_chamfer_losses, label=\"Test Chamfer\", linewidth=2, color='red', alpha=0.8, linestyle='--')\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.set_ylabel(\"Chamfer Distance\")\n",
        "    ax2.set_title(\"Geometric Accuracy Evolution\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_yscale('log')\n",
        "\n",
        "    # 3. Learning Rate Schedule\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    ax3.plot(learning_rates, linewidth=2, color='green')\n",
        "    ax3.set_xlabel(\"Epoch\")\n",
        "    ax3.set_ylabel(\"Learning Rate\")\n",
        "    ax3.set_title(\"Learning Rate Schedule\")\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.set_yscale('log')\n",
        "\n",
        "    # 4. Loss Smoothing (Moving Average)\n",
        "    window_size = 5\n",
        "    ax4 = fig.add_subplot(gs[1, 0])\n",
        "    if len(train_losses) >= window_size:\n",
        "        train_smooth = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        val_smooth = np.convolve(val_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "        test_smooth = np.convolve(test_losses, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "        epochs_smooth = range(window_size-1, len(train_losses))\n",
        "        ax4.plot(epochs_smooth, train_smooth, label=\"Train (Smoothed)\", linewidth=2, color='blue')\n",
        "        ax4.plot(epochs_smooth, val_smooth, label=\"Val (Smoothed)\", linewidth=2, color='orange')\n",
        "        ax4.plot(epochs_smooth, test_smooth, label=\"Test (Smoothed)\", linewidth=2, color='red', linestyle='--')\n",
        "    ax4.set_xlabel(\"Epoch\")\n",
        "    ax4.set_ylabel(\"Smoothed Loss\")\n",
        "    ax4.set_title(\"Loss Trends (Moving Average)\")\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Validation vs Training Loss Ratio\n",
        "    ax5 = fig.add_subplot(gs[1, 1])\n",
        "    if len(train_losses) > 0 and len(val_losses) > 0:\n",
        "        loss_ratio = np.array(val_losses) / np.array(train_losses)\n",
        "        ax5.plot(loss_ratio, linewidth=2, color='purple')\n",
        "        ax5.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Perfect Generalization')\n",
        "        ax5.set_xlabel(\"Epoch\")\n",
        "        ax5.set_ylabel(\"Val Loss / Train Loss\")\n",
        "        ax5.set_title(\"Generalization Gap\")\n",
        "        ax5.legend()\n",
        "        ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. Loss Distribution\n",
        "    ax6 = fig.add_subplot(gs[1, 2])\n",
        "    if len(train_losses) > 10:\n",
        "        ax6.hist(train_losses[-20:], alpha=0.7, label='Train (Last 20)', bins=10, color='blue')\n",
        "        ax6.hist(val_losses[-20:], alpha=0.7, label='Val (Last 20)', bins=10, color='orange')\n",
        "        ax6.hist(test_losses[-20:], alpha=0.7, label='Test (Last 20)', bins=10, color='red')\n",
        "    ax6.set_xlabel(\"Loss Value\")\n",
        "    ax6.set_ylabel(\"Frequency\")\n",
        "    ax6.set_title(\"Recent Loss Distribution\")\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    # 7. Performance Improvement Rate\n",
        "    ax7 = fig.add_subplot(gs[2, 0])\n",
        "    if len(val_losses) > 1:\n",
        "        improvement_rate = np.diff(val_losses)\n",
        "        ax7.plot(improvement_rate, linewidth=2, color='green')\n",
        "        ax7.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
        "        ax7.set_xlabel(\"Epoch\")\n",
        "        ax7.set_ylabel(\"Loss Change\")\n",
        "        ax7.set_title(\"Validation Loss Improvement Rate\")\n",
        "        ax7.grid(True, alpha=0.3)\n",
        "\n",
        "    # 8. Cumulative Best Performance\n",
        "    ax8 = fig.add_subplot(gs[2, 1])\n",
        "    val_best_cumulative = np.minimum.accumulate(val_losses)\n",
        "    test_best_cumulative = np.minimum.accumulate(test_losses)\n",
        "    ax8.plot(val_best_cumulative, label=\"Best Val Loss\", linewidth=2, color='orange')\n",
        "    ax8.plot(test_best_cumulative, label=\"Best Test Loss\", linewidth=2, color='red')\n",
        "    ax8.set_xlabel(\"Epoch\")\n",
        "    ax8.set_ylabel(\"Best Loss So Far\")\n",
        "    ax8.set_title(\"Cumulative Best Performance\")\n",
        "    ax8.legend()\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    ax8.set_yscale('log')\n",
        "\n",
        "    # 9. Training Summary Statistics\n",
        "    ax9 = fig.add_subplot(gs[2, 2])\n",
        "    ax9.axis('off')\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    final_train_loss = train_losses[-1] if train_losses else 0\n",
        "    final_val_loss = val_losses[-1] if val_losses else 0\n",
        "    final_test_loss = test_losses[-1] if test_losses else 0\n",
        "    best_val_epoch = np.argmin(val_losses) + 1 if val_losses else 0\n",
        "    best_test_epoch = np.argmin(test_losses) + 1 if test_losses else 0\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "Training Summary\n",
        "\n",
        "Total Epochs: {epoch}\n",
        "Best Val Loss: {best_val_loss:.6f} (Epoch {best_val_epoch})\n",
        "Best Test Loss: {best_test_loss:.6f} (Epoch {best_test_epoch})\n",
        "\n",
        "Final Performance:\n",
        "Train Loss: {final_train_loss:.6f}\n",
        "Val Loss: {final_val_loss:.6f}\n",
        "Test Loss: {final_test_loss:.6f}\n",
        "\n",
        "Improvement:\n",
        "Val: {((val_losses[0] - best_val_loss) / val_losses[0] * 100):.1f}%\n",
        "Test: {((test_losses[0] - best_test_loss) / test_losses[0] * 100):.1f}%\n",
        "    \"\"\"\n",
        "\n",
        "    ax9.text(0.1, 0.9, summary_text, transform=ax9.transAxes, fontsize=12,\n",
        "             verticalalignment='top', fontfamily='monospace',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "\n",
        "    plt.suptitle(\"3D Dental Prosthetic Generator - Training Analysis\", fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_analysis_comprehensive.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ixrrAR9endb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Functions**"
      ],
      "metadata": {
        "id": "En4X5roMxlom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, test_loss,\n",
        "                         best_val_loss, filepath, additional_info=None):\n",
        "    \"\"\"\n",
        "    Save comprehensive model checkpoint with all training state\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'test_loss': test_loss,\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'model_architecture': str(model),\n",
        "        'timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'device': str(next(model.parameters()).device),\n",
        "        'pytorch_version': torch.__version__,\n",
        "    }\n",
        "\n",
        "    if additional_info:\n",
        "        checkpoint.update(additional_info)\n",
        "\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Checkpoint saved to {filepath}\")\n",
        "\n",
        "    # Also save model architecture separately for deployment\n",
        "    model_only_path = filepath.replace('.pth', '_model_only.pth')\n",
        "    torch.save(model.state_dict(), model_only_path)\n",
        "\n",
        "def load_model_checkpoint(model, optimizer, scheduler, filepath, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load comprehensive model checkpoint and restore training state\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    if scheduler and checkpoint.get('scheduler_state_dict'):\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    print(f\"Checkpoint loaded from {filepath}\")\n",
        "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
        "    print(f\"  Best Val Loss: {checkpoint['best_val_loss']:.6f}\")\n",
        "    print(f\"  Saved on: {checkpoint.get('timestamp', 'Unknown')}\")\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "def save_training_history(train_losses, val_losses, test_losses, additional_metrics=None):\n",
        "    \"\"\"\n",
        "    Save training history as JSON and CSV for analysis\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'test_losses': test_losses,\n",
        "        'epochs': list(range(1, len(train_losses) + 1)),\n",
        "        'timestamp': pd.Timestamp.now().isoformat()\n",
        "    }\n",
        "\n",
        "    if additional_metrics:\n",
        "        history.update(additional_metrics)\n",
        "\n",
        "    # Save as JSON\n",
        "    with open('training_history.json', 'w') as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    # Save as CSV for easy analysis\n",
        "    df = pd.DataFrame({\n",
        "        'epoch': history['epochs'],\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses,\n",
        "        'test_loss': test_losses\n",
        "    })\n",
        "\n",
        "    if additional_metrics:\n",
        "        for key, values in additional_metrics.items():\n",
        "            if isinstance(values, list) and len(values) == len(train_losses):\n",
        "                df[key] = values\n",
        "\n",
        "    df.to_csv('training_history.csv', index=False)\n",
        "    print(\"Training history saved to training_history.json and training_history.csv\")\n"
      ],
      "metadata": {
        "id": "am-5okIixoHH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}